{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "#LOCAL IMPORTS\n",
    "\n",
    "\n",
    "#GLOBAL VARIABLES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10381</th>\n",
       "      <td>It would seem that the weather is improving.</td>\n",
       "      <td>يظهر أن الطقس يتحسن.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7979</th>\n",
       "      <td>They accused me of being a liar.</td>\n",
       "      <td>هم اتهموني بالكذب.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>It may rain.</td>\n",
       "      <td>من الممكن أن تمطر.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11030</th>\n",
       "      <td>We tried to get him to change his mind, but co...</td>\n",
       "      <td>حاولنا أن نغير رأيه لكننا لم نستطع.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7914</th>\n",
       "      <td>She showed me around the campus.</td>\n",
       "      <td>أعطتني جولة في الحرم الجامعي.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>He knows the city well.</td>\n",
       "      <td>هو يعرف المدينة جيداً.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "10381       It would seem that the weather is improving.   \n",
       "7979                    They accused me of being a liar.   \n",
       "397                                         It may rain.   \n",
       "11030  We tried to get him to change his mind, but co...   \n",
       "7914                    She showed me around the campus.   \n",
       "4133                             He knows the city well.   \n",
       "\n",
       "                                    target  \\\n",
       "10381                 يظهر أن الطقس يتحسن.   \n",
       "7979                    هم اتهموني بالكذب.   \n",
       "397                     من الممكن أن تمطر.   \n",
       "11030  حاولنا أن نغير رأيه لكننا لم نستطع.   \n",
       "7914         أعطتني جولة في الحرم الجامعي.   \n",
       "4133                هو يعرف المدينة جيداً.   \n",
       "\n",
       "                                                comments  \n",
       "10381  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "7979   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "397    CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "11030  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "7914   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "4133   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data():\n",
    "    #Fetch THE DATA\n",
    "    path_to_data = \"dataset/ara.txt\"\n",
    "\n",
    "    #Retrive some of the data\n",
    "    lines = pd.read_table(path_to_data, names = ['source', 'target', 'comments'])\n",
    "    return lines\n",
    "\n",
    "lines = read_data()\n",
    "lines.sample(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>&lt;sos&gt; a good idea occurred to me &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; خطر لي فكرة جيدة &lt;eos&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>&lt;sos&gt; there are many hotels downtown &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; هناك الكثير من الفنادق في وسط المدينة &lt;eos&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10356</th>\n",
       "      <td>&lt;sos&gt; how often do you forget to do your homew...</td>\n",
       "      <td>&lt;sos&gt; أتنسى عادة القيام بواجبات &lt;eos&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>&lt;sos&gt; what browser are you using &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; ما البرنامج الذي تتصفح به الإنترنت &lt;eos&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>&lt;sos&gt; im sad &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; أنا حزين &lt;eos&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9781</th>\n",
       "      <td>&lt;sos&gt; when was the last time you rode a bike &lt;...</td>\n",
       "      <td>&lt;sos&gt; متى كانت آخر مرة ركبت بها دراجة هوائية &lt;...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source  \\\n",
       "5755              <sos> a good idea occurred to me <eos>   \n",
       "7525          <sos> there are many hotels downtown <eos>   \n",
       "10356  <sos> how often do you forget to do your homew...   \n",
       "6062              <sos> what browser are you using <eos>   \n",
       "47                                    <sos> im sad <eos>   \n",
       "9781   <sos> when was the last time you rode a bike <...   \n",
       "\n",
       "                                                  target  \\\n",
       "5755                        <sos> خطر لي فكرة جيدة <eos>   \n",
       "7525   <sos> هناك الكثير من الفنادق في وسط المدينة <eos>   \n",
       "10356              <sos> أتنسى عادة القيام بواجبات <eos>   \n",
       "6062      <sos> ما البرنامج الذي تتصفح به الإنترنت <eos>   \n",
       "47                                  <sos> أنا حزين <eos>   \n",
       "9781   <sos> متى كانت آخر مرة ركبت بها دراجة هوائية <...   \n",
       "\n",
       "                                                comments  \n",
       "5755   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "7525   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "10356  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "6062   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "47     CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "9781   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Cleaning the data:\n",
    "        - Convert english (source) into lowercase\n",
    "        - Remove Qoutes\n",
    "        - Remove all special characters like “@, !, *, $, #, ?, %, etc.”\n",
    "        - Remove numbers .. since they are different!\n",
    "        - Remove spaces\n",
    "'''\n",
    "#Read the data\n",
    "lines = read_data()\n",
    "#Shuffle the data\n",
    "lines = shuffle(lines)\n",
    "\n",
    "#LOWER CASE\n",
    "lines.source = lines.source.apply (lambda x: x.lower())\n",
    "# lines.target = lines.target.apply (lambda x: x.lower()) --ARABIC\n",
    "\n",
    "#Qoutes\n",
    "lines.source = lines.source.apply (lambda x: re.sub(\"'\", '', x))\n",
    "lines.target = lines.target.apply (lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "#A list of all punctuations\n",
    "punc = set(string.punctuation)\n",
    "\n",
    "punc.add ('؟') #ara\n",
    "punc.add ('...') #ara\n",
    "punc.add ('...') #eng\n",
    "punc.add ('،') #ara\n",
    "lines.source = lines.source.apply(lambda x: ''.join(char1 for char1 in x if char1 not in punc))\n",
    "lines.target = lines.target.apply(lambda x: ''.join(char1 for char1 in x if char1 not in punc))\n",
    "\n",
    "#Remving digits\n",
    "num_digits= str.maketrans('','', digits)\n",
    "lines.source = lines.source.apply(lambda x: x.translate(num_digits))\n",
    "lines.target = lines.target.apply(lambda x: x.translate(num_digits))\n",
    "\n",
    "# Remove extra spaces\n",
    "lines.source=lines.source.apply(lambda x: x.strip())\n",
    "lines.target=lines.target.apply(lambda x: x.strip())\n",
    "lines.source=lines.source.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines.target=lines.target.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "#adding start/end tags\n",
    "lines.source = lines.source.apply(lambda x : '<sos> '+ x + ' <eos>')\n",
    "lines.target = lines.target.apply(lambda x : '<sos> '+ x + ' <eos>')\n",
    "lines.sample(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Whole Source count \t\t65184\n",
      "Uniques in the Source count \t6902\n",
      "\n",
      "Uniques in the Target count \t14443\n"
     ]
    }
   ],
   "source": [
    "#The whole Source set length\n",
    "source_vocab = []\n",
    "for line in lines.source:\n",
    "    chars = line.split(' ')\n",
    "    for char in chars: \n",
    "        source_vocab.append (char)\n",
    "\n",
    "target_vocab = set()\n",
    "for line in lines.target:\n",
    "    chars = line.split(' ')\n",
    "    for char in chars: \n",
    "        if char not in target_vocab:\n",
    "            target_vocab.add(char)\n",
    "        \n",
    "print (f'The Whole Source count \\t\\t{len(source_vocab)}')\n",
    "print (f'Uniques in the Source count \\t{len(set(source_vocab))}')\n",
    "print ()\n",
    "print (f'Uniques in the Target count \\t{len(target_vocab)}')\n",
    "#NOTE: the arabic language is vastly rich!\n",
    "\n",
    "source_vocab = sorted(list(set(source_vocab)))\n",
    "target_vocab = sorted(list(target_vocab))\n",
    "\n",
    "# Input tokens for encoder\n",
    "num_encoder_tokens=len(set(source_vocab))\n",
    "# Input tokens for decoder zero padded\n",
    "num_decoder_tokens=len(target_vocab) +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of Source lang: 34\n",
      "Max length of Target lang: 36\n"
     ]
    }
   ],
   "source": [
    "#Max lengths for Source and Targets\n",
    "source_length_list=[]\n",
    "for l in lines.source:\n",
    "    source_length_list.append(len(l.split(' ')))\n",
    "src_max_length= max(source_length_list)\n",
    "\n",
    "target_length_list=[]\n",
    "for l in lines.target:\n",
    "    target_length_list.append(len(l.split(' ')))\n",
    "trg_max_length= max(target_length_list)\n",
    "\n",
    "print (f'Max length of Source lang: {src_max_length}')\n",
    "print (f'Max length of Target lang: {trg_max_length}')\n",
    "\n",
    "#Word to index dictionary\n",
    "source_word2idx= dict([(word, i+1) for i,word in enumerate(source_vocab)])\n",
    "target_word2idx=dict([(word, i+1) for i, word in enumerate(target_vocab)])\n",
    "\n",
    "#creating a dictionary for index to word for source and target vocabulary\n",
    "source_idx2word= dict([(i, word) for word, i in  source_word2idx.items()])\n",
    "target_idx2word =dict([(i, word) for word, i in target_word2idx.items()])\n",
    "\n",
    "#Shuffle the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Training data shape: \t(10301,)\n",
      "Target Training data shape: \t(10301,)\n",
      "\n",
      "Source Test data shape: \t(1145,)\n",
      "Target Test data shape: \t(1145,)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data\n",
    "#train_test_split from Sklearn lib\n",
    "lines = read_data()\n",
    "lines = shuffle(lines)\n",
    "\n",
    "X, y = shuffle(np.array(lines.source)), shuffle(np.array(lines.target))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "print (f'Source Training data shape: \\t{X_train.shape}')\n",
    "print (f'Target Training data shape: \\t{y_train.shape}')\n",
    "print ()\n",
    "print (f'Source Test data shape: \\t{X_test.shape}')\n",
    "print (f'Target Test data shape: \\t{y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_batch() will provide batches of data for fit_generator()\n",
    "\n",
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, src_max_length),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, trg_max_length),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, trg_max_length, num_decoder_tokens),dtype='float32')\n",
    "            \n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    try:\n",
    "                        encoder_input_data[i, t] = source_word2idx[word] \n",
    "                    except:\n",
    "                        print (encoder_input_data)\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_word2idx[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        #print(word)\n",
    "                        decoder_target_data[i, t - 1, target_word2idx[word]] = 1.\n",
    "                    \n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL\n",
    "\n",
    "#Essenteal parameters\n",
    "train_samples = len(X_train)\n",
    "test_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "latent_dim=256\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that takes encoder and decoder input \n",
    "# to output decoder_outputs\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "#optimizer: rmsprop .. try adam!\n",
    "#loss: cross entropy\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = test_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(‘nmt_weights_100epochs.h5’)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
