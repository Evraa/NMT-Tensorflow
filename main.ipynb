{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "#GLOBAL IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "#LOCAL IMPORTS\n",
    "from workspace_utils import active_session\n",
    "if tf.test.is_gpu_available:\n",
    "    print (\"GPU\")\n",
    "\n",
    "#GLOBAL VARIABLES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6696</th>\n",
       "      <td>Tom knows the city very well.</td>\n",
       "      <td>يعرف توم المدينة جيدا</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>Don't worry about it!</td>\n",
       "      <td>لا تقلق بشأن ذلك!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>No one helped me.</td>\n",
       "      <td>لم يساعدني أحد.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>Are you new, too?</td>\n",
       "      <td>هل أنت جديد أيضاً؟</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6968</th>\n",
       "      <td>My hands and legs are swollen.</td>\n",
       "      <td>تورمت قدميّ و يديّ.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8469</th>\n",
       "      <td>Would you mind shutting the door?</td>\n",
       "      <td>أيمكنك أن تقفل الباب؟</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 source                 target  \\\n",
       "6696      Tom knows the city very well.  يعرف توم المدينة جيدا   \n",
       "3140              Don't worry about it!      لا تقلق بشأن ذلك!   \n",
       "1614                  No one helped me.        لم يساعدني أحد.   \n",
       "1406                  Are you new, too?     هل أنت جديد أيضاً؟   \n",
       "6968     My hands and legs are swollen.    تورمت قدميّ و يديّ.   \n",
       "8469  Would you mind shutting the door?  أيمكنك أن تقفل الباب؟   \n",
       "\n",
       "                                               comments  \n",
       "6696  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "3140  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "1614  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "1406  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "6968  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "8469  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data():\n",
    "    #Fetch THE DATA\n",
    "    path_to_data = \"ara.txt\"\n",
    "\n",
    "    #Retrive some of the data\n",
    "    lines = pd.read_table(path_to_data, names = ['source', 'target', 'comments'])\n",
    "    return lines\n",
    "\n",
    "lines = read_data()\n",
    "lines.sample(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>&lt;sos&gt; your dog is here &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; هنا كلبك &lt;eos&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5063</th>\n",
       "      <td>&lt;sos&gt; im glad to see you back &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; سررت بعودتك &lt;eos&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7780</th>\n",
       "      <td>&lt;sos&gt; im very grateful for your help &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; أنا ممتن للغاية لمساعدتك &lt;eos&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8041</th>\n",
       "      <td>&lt;sos&gt; why couldnt you come yesterday &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; لماذا لم تستطع القدوم بالأمس &lt;eos&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>&lt;sos&gt; i am cold &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; أشعر بالبرد &lt;eos&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4758</th>\n",
       "      <td>&lt;sos&gt; tom almost convinced me &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; أقنعني توم تقريباً &lt;eos&gt;</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source  \\\n",
       "1791                <sos> your dog is here <eos>   \n",
       "5063         <sos> im glad to see you back <eos>   \n",
       "7780  <sos> im very grateful for your help <eos>   \n",
       "8041  <sos> why couldnt you come yesterday <eos>   \n",
       "138                        <sos> i am cold <eos>   \n",
       "4758         <sos> tom almost convinced me <eos>   \n",
       "\n",
       "                                        target  \\\n",
       "1791                      <sos> هنا كلبك <eos>   \n",
       "5063                   <sos> سررت بعودتك <eos>   \n",
       "7780      <sos> أنا ممتن للغاية لمساعدتك <eos>   \n",
       "8041  <sos> لماذا لم تستطع القدوم بالأمس <eos>   \n",
       "138                    <sos> أشعر بالبرد <eos>   \n",
       "4758            <sos> أقنعني توم تقريباً <eos>   \n",
       "\n",
       "                                               comments  \n",
       "1791  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "5063  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "7780  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "8041  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "138   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "4758  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Cleaning the data:\n",
    "        - Convert english (source) into lowercase\n",
    "        - Remove Qoutes\n",
    "        - Remove all special characters like “@, !, *, $, #, ?, %, etc.”\n",
    "        - Remove numbers .. since they are different!\n",
    "        - Remove spaces\n",
    "'''\n",
    "#Read the data\n",
    "lines = read_data()\n",
    "#Shuffle the data\n",
    "lines = shuffle(lines)\n",
    "\n",
    "#LOWER CASE\n",
    "lines.source = lines.source.apply (lambda x: x.lower())\n",
    "lines.target = lines.target.apply (lambda x: x.lower()) \n",
    "\n",
    "#Qoutes\n",
    "lines.source = lines.source.apply (lambda x: re.sub(\"'\", '', x))\n",
    "lines.target = lines.target.apply (lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "#A list of all punctuations\n",
    "punc = set(string.punctuation)\n",
    "\n",
    "punc.add ('؟') #ara\n",
    "punc.add ('...') #ara\n",
    "punc.add ('...') #eng\n",
    "punc.add ('،') #ara\n",
    "lines.source = lines.source.apply(lambda x: ''.join(char1 for char1 in x if char1 not in punc))\n",
    "lines.target = lines.target.apply(lambda x: ''.join(char1 for char1 in x if char1 not in punc))\n",
    "\n",
    "#Remving digits\n",
    "num_digits= str.maketrans('','', digits)\n",
    "lines.source = lines.source.apply(lambda x: x.translate(num_digits))\n",
    "lines.target = lines.target.apply(lambda x: x.translate(num_digits))\n",
    "\n",
    "# Remove extra spaces\n",
    "lines.source=lines.source.apply(lambda x: x.strip())\n",
    "lines.target=lines.target.apply(lambda x: x.strip())\n",
    "lines.source=lines.source.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines.target=lines.target.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "#adding start/end tags\n",
    "lines.source = lines.source.apply(lambda x : '<sos> '+ x + ' <eos>')\n",
    "lines.target = lines.target.apply(lambda x : '<sos> '+ x + ' <eos>')\n",
    "lines.sample(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniques in the Source count \t4168\n",
      "\n",
      "Uniques in the Target count \t11774\n"
     ]
    }
   ],
   "source": [
    "#The whole Source set length\n",
    "\n",
    "source_vocab = set()\n",
    "for line in lines.source:\n",
    "    chars = line.split(' ')\n",
    "    for char in chars: \n",
    "        if char not in source_vocab:\n",
    "            source_vocab.add(char)\n",
    "            \n",
    "target_vocab = set()\n",
    "for line in lines.target:\n",
    "    chars = line.split(' ')\n",
    "    for char in chars: \n",
    "        if char not in target_vocab:\n",
    "            target_vocab.add(char)\n",
    "        \n",
    "print (f'Uniques in the Source count \\t{len(source_vocab)}')\n",
    "print ()\n",
    "print (f'Uniques in the Target count \\t{len(target_vocab)}')\n",
    "#NOTE: the arabic language is vastly rich!\n",
    "\n",
    "source_vocab = sorted(list(source_vocab))\n",
    "target_vocab = sorted(list(target_vocab))\n",
    "\n",
    "# Input tokens for encoder\n",
    "num_encoder_tokens=len(source_vocab)\n",
    "# Input tokens for decoder zero padded\n",
    "num_decoder_tokens=len(target_vocab) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of Source lang: 36\n",
      "Max length of Target lang: 38\n"
     ]
    }
   ],
   "source": [
    "#Max lengths for Source and Targets\n",
    "source_length_list=[]\n",
    "for l in lines.source:\n",
    "    source_length_list.append(len(l.split(' ')))\n",
    "src_max_length= max(source_length_list)\n",
    "\n",
    "target_length_list=[]\n",
    "for l in lines.target:\n",
    "    target_length_list.append(len(l.split(' ')))\n",
    "trg_max_length= max(target_length_list)\n",
    "\n",
    "print (f'Max length of Source lang: {src_max_length}')\n",
    "print (f'Max length of Target lang: {trg_max_length}')\n",
    "\n",
    "#Word to index dictionary\n",
    "source_word2idx= dict([(word, i+1) for i,word in enumerate(source_vocab)])\n",
    "target_word2idx=dict([(word, i+1) for i, word in enumerate(target_vocab)])\n",
    "\n",
    "#creating a dictionary for index to word for source and target vocabulary\n",
    "source_idx2word= dict([(i, word) for word, i in  source_word2idx.items()])\n",
    "target_idx2word =dict([(i, word) for word, i in target_word2idx.items()])\n",
    "\n",
    "#Shuffle the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Training data shape: \t(10301,)\n",
      "Target Training data shape: \t(10301,)\n",
      "\n",
      "Source Test data shape: \t(1145,)\n",
      "Target Test data shape: \t(1145,)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data\n",
    "#train_test_split from Sklearn lib\n",
    "\n",
    "X, y = lines.source, lines.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "print (f'Source Training data shape: \\t{X_train.shape}')\n",
    "print (f'Target Training data shape: \\t{y_train.shape}')\n",
    "print ()\n",
    "print (f'Source Test data shape: \\t{X_test.shape}')\n",
    "print (f'Target Test data shape: \\t{y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_batch() will provide batches of data for fit_generator()\n",
    "\n",
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, src_max_length),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, trg_max_length),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, trg_max_length, num_decoder_tokens),dtype='float32')\n",
    "            \n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split(' ')):\n",
    "                    encoder_input_data[i, t] = source_word2idx[word] \n",
    "                for t, word in enumerate(target_text.split(' ')):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_word2idx[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        #print(word)\n",
    "                        decoder_target_data[i, t - 1, target_word2idx[word]] = 1.\n",
    "                    \n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL\n",
    "\n",
    "#Essenteal parameters\n",
    "train_samples = len(X_train)\n",
    "test_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 65\n",
    "latent_dim=256\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that takes encoder and decoder input \n",
    "# to output decoder_outputs\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "#optimizer: rmsprop .. try adam!\n",
    "#loss: cross entropy\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "80/80 [==============================] - 65s 813ms/step - loss: 6.9097 - acc: 0.1959 - val_loss: 6.6283 - val_acc: 0.2061\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 6.3271 - acc: 0.2098 - val_loss: 6.6216 - val_acc: 0.2164\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 62s 770ms/step - loss: 6.0551 - acc: 0.2196 - val_loss: 6.5504 - val_acc: 0.2271\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 62s 775ms/step - loss: 5.8104 - acc: 0.2268 - val_loss: 6.6461 - val_acc: 0.2268\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 5.5926 - acc: 0.2339 - val_loss: 6.5955 - val_acc: 0.2254\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 62s 777ms/step - loss: 5.3885 - acc: 0.2395 - val_loss: 6.5407 - val_acc: 0.2352\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 5.1869 - acc: 0.2470 - val_loss: 6.4970 - val_acc: 0.2408\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 61s 763ms/step - loss: 4.9966 - acc: 0.2574 - val_loss: 6.4578 - val_acc: 0.2475\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 61s 768ms/step - loss: 4.8042 - acc: 0.2687 - val_loss: 6.3911 - val_acc: 0.2513\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 61s 769ms/step - loss: 4.6193 - acc: 0.2805 - val_loss: 6.3095 - val_acc: 0.2632\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 62s 776ms/step - loss: 4.4277 - acc: 0.2953 - val_loss: 6.2781 - val_acc: 0.2710\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 62s 772ms/step - loss: 4.2441 - acc: 0.3100 - val_loss: 6.2093 - val_acc: 0.2767\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 61s 768ms/step - loss: 4.0665 - acc: 0.3257 - val_loss: 6.1947 - val_acc: 0.2736\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 62s 769ms/step - loss: 3.8937 - acc: 0.3417 - val_loss: 6.1424 - val_acc: 0.2833\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 3.7243 - acc: 0.3581 - val_loss: 6.0991 - val_acc: 0.2919\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 62s 770ms/step - loss: 3.5568 - acc: 0.3757 - val_loss: 6.0472 - val_acc: 0.2968\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 3.3881 - acc: 0.3954 - val_loss: 5.9935 - val_acc: 0.3032\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 59s 743ms/step - loss: 3.2283 - acc: 0.4178 - val_loss: 5.9602 - val_acc: 0.3085\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 60s 751ms/step - loss: 3.0730 - acc: 0.4398 - val_loss: 5.9493 - val_acc: 0.3093\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 62s 769ms/step - loss: 2.9148 - acc: 0.4634 - val_loss: 5.9483 - val_acc: 0.3042\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 2.7663 - acc: 0.4877 - val_loss: 5.9489 - val_acc: 0.3080\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 61s 762ms/step - loss: 2.6170 - acc: 0.5127 - val_loss: 5.9198 - val_acc: 0.3097\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 61s 764ms/step - loss: 2.4801 - acc: 0.5376 - val_loss: 5.8960 - val_acc: 0.3192\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 61s 767ms/step - loss: 2.3392 - acc: 0.5642 - val_loss: 5.8863 - val_acc: 0.3174\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 61s 763ms/step - loss: 2.2083 - acc: 0.5899 - val_loss: 5.8899 - val_acc: 0.3167\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 2.0736 - acc: 0.6157 - val_loss: 5.9234 - val_acc: 0.3065\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 61s 765ms/step - loss: 1.9403 - acc: 0.6437 - val_loss: 5.9012 - val_acc: 0.3157\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 61s 766ms/step - loss: 1.8219 - acc: 0.6685 - val_loss: 5.8739 - val_acc: 0.3207\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 61s 767ms/step - loss: 1.7003 - acc: 0.6943 - val_loss: 5.8654 - val_acc: 0.3256\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 61s 764ms/step - loss: 1.5870 - acc: 0.7175 - val_loss: 5.8916 - val_acc: 0.3200\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 62s 778ms/step - loss: 1.4755 - acc: 0.7417 - val_loss: 5.8621 - val_acc: 0.3329\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 62s 769ms/step - loss: 1.3720 - acc: 0.7623 - val_loss: 5.8742 - val_acc: 0.3320\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 61s 766ms/step - loss: 1.2710 - acc: 0.7841 - val_loss: 5.8840 - val_acc: 0.3367\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 61s 767ms/step - loss: 1.1734 - acc: 0.8050 - val_loss: 5.9064 - val_acc: 0.3305\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 60s 754ms/step - loss: 1.0880 - acc: 0.8217 - val_loss: 5.9214 - val_acc: 0.3284\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 62s 775ms/step - loss: 1.0032 - acc: 0.8397 - val_loss: 5.9398 - val_acc: 0.3283\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 61s 758ms/step - loss: 0.9234 - acc: 0.8558 - val_loss: 5.9645 - val_acc: 0.3309\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 61s 761ms/step - loss: 0.8510 - acc: 0.8690 - val_loss: 5.9589 - val_acc: 0.3357\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 61s 762ms/step - loss: 0.7789 - acc: 0.8836 - val_loss: 5.9674 - val_acc: 0.3385\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 61s 765ms/step - loss: 0.7090 - acc: 0.8964 - val_loss: 5.9791 - val_acc: 0.3357\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.6522 - acc: 0.9079 - val_loss: 6.0125 - val_acc: 0.3315\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 61s 768ms/step - loss: 0.5967 - acc: 0.9176 - val_loss: 6.0166 - val_acc: 0.3325\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 62s 773ms/step - loss: 0.5381 - acc: 0.9268 - val_loss: 6.0277 - val_acc: 0.3329\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 61s 767ms/step - loss: 0.4924 - acc: 0.9349 - val_loss: 6.0122 - val_acc: 0.3451\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 61s 759ms/step - loss: 0.4457 - acc: 0.9423 - val_loss: 6.0416 - val_acc: 0.3441\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.4015 - acc: 0.9494 - val_loss: 6.1041 - val_acc: 0.3315\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 0.3660 - acc: 0.9551 - val_loss: 6.1235 - val_acc: 0.3342\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 61s 765ms/step - loss: 0.3283 - acc: 0.9601 - val_loss: 6.1509 - val_acc: 0.3300\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 61s 767ms/step - loss: 0.2972 - acc: 0.9651 - val_loss: 6.1470 - val_acc: 0.3337\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 61s 767ms/step - loss: 0.2675 - acc: 0.9703 - val_loss: 6.1611 - val_acc: 0.3350\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 62s 779ms/step - loss: 0.2400 - acc: 0.9729 - val_loss: 6.2181 - val_acc: 0.3284\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 61s 766ms/step - loss: 0.2177 - acc: 0.9761 - val_loss: 6.2212 - val_acc: 0.3318\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 61s 764ms/step - loss: 0.1964 - acc: 0.9789 - val_loss: 6.2347 - val_acc: 0.3319\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 0.1767 - acc: 0.9812 - val_loss: 6.2545 - val_acc: 0.3377\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 61s 762ms/step - loss: 0.1604 - acc: 0.9832 - val_loss: 6.2688 - val_acc: 0.3356\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 62s 771ms/step - loss: 0.1441 - acc: 0.9850 - val_loss: 6.3173 - val_acc: 0.3290\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 61s 762ms/step - loss: 0.1304 - acc: 0.9862 - val_loss: 6.2985 - val_acc: 0.3330\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 0.1192 - acc: 0.9875 - val_loss: 6.3126 - val_acc: 0.3369\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 61s 764ms/step - loss: 0.1082 - acc: 0.9881 - val_loss: 6.3168 - val_acc: 0.3356\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 61s 762ms/step - loss: 0.0976 - acc: 0.9895 - val_loss: 6.3560 - val_acc: 0.3334\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 62s 774ms/step - loss: 0.0880 - acc: 0.9903 - val_loss: 6.3428 - val_acc: 0.3392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "80/80 [==============================] - 61s 764ms/step - loss: 0.0813 - acc: 0.9910 - val_loss: 6.3922 - val_acc: 0.3299\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 61s 767ms/step - loss: 0.0742 - acc: 0.9910 - val_loss: 6.4232 - val_acc: 0.3305\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 0.0715 - acc: 0.9913 - val_loss: 6.4412 - val_acc: 0.3333\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 0.0610 - acc: 0.9924 - val_loss: 6.4364 - val_acc: 0.3384\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 61s 759ms/step - loss: 0.0601 - acc: 0.9927 - val_loss: 6.4606 - val_acc: 0.3356\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 61s 766ms/step - loss: 0.0566 - acc: 0.9926 - val_loss: 6.4754 - val_acc: 0.3354\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 60s 756ms/step - loss: 0.0525 - acc: 0.9927 - val_loss: 6.4801 - val_acc: 0.3334\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 61s 758ms/step - loss: 0.0480 - acc: 0.9931 - val_loss: 6.4914 - val_acc: 0.3339\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 0.0466 - acc: 0.9933 - val_loss: 6.4907 - val_acc: 0.3344\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 61s 767ms/step - loss: 0.0435 - acc: 0.9933 - val_loss: 6.5257 - val_acc: 0.3316\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 61s 764ms/step - loss: 0.0436 - acc: 0.9932 - val_loss: 6.5635 - val_acc: 0.3269\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 61s 756ms/step - loss: 0.0402 - acc: 0.9933 - val_loss: 6.6029 - val_acc: 0.3234\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 61s 758ms/step - loss: 0.0374 - acc: 0.9936 - val_loss: 6.5603 - val_acc: 0.3344\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 0.0366 - acc: 0.9934 - val_loss: 6.5872 - val_acc: 0.3322\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 61s 761ms/step - loss: 0.0368 - acc: 0.9933 - val_loss: 6.6203 - val_acc: 0.3307\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 61s 765ms/step - loss: 0.0334 - acc: 0.9937 - val_loss: 6.6237 - val_acc: 0.3307\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 0.0327 - acc: 0.9938 - val_loss: 6.6499 - val_acc: 0.3257\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 61s 761ms/step - loss: 0.0335 - acc: 0.9936 - val_loss: 6.6045 - val_acc: 0.3357\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 61s 758ms/step - loss: 0.0305 - acc: 0.9940 - val_loss: 6.6624 - val_acc: 0.3309\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 61s 761ms/step - loss: 0.0285 - acc: 0.9944 - val_loss: 6.6153 - val_acc: 0.3345\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 61s 764ms/step - loss: 0.0305 - acc: 0.9937 - val_loss: 6.6815 - val_acc: 0.3328\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 61s 762ms/step - loss: 0.0301 - acc: 0.9936 - val_loss: 6.6694 - val_acc: 0.3330\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 61s 758ms/step - loss: 0.0301 - acc: 0.9936 - val_loss: 6.6779 - val_acc: 0.3324\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 61s 756ms/step - loss: 0.0281 - acc: 0.9939 - val_loss: 6.7151 - val_acc: 0.3304\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 61s 759ms/step - loss: 0.0283 - acc: 0.9936 - val_loss: 6.7405 - val_acc: 0.3317\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 61s 764ms/step - loss: 0.0264 - acc: 0.9940 - val_loss: 6.7479 - val_acc: 0.3295\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 61s 758ms/step - loss: 0.0279 - acc: 0.9937 - val_loss: 6.7318 - val_acc: 0.3298\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.0273 - acc: 0.9937 - val_loss: 6.7713 - val_acc: 0.3276\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 61s 759ms/step - loss: 0.0249 - acc: 0.9941 - val_loss: 6.7751 - val_acc: 0.3278\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 61s 766ms/step - loss: 0.0251 - acc: 0.9941 - val_loss: 6.7807 - val_acc: 0.3260\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 61s 765ms/step - loss: 0.0266 - acc: 0.9938 - val_loss: 6.8171 - val_acc: 0.3255\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 0.0252 - acc: 0.9941 - val_loss: 6.8104 - val_acc: 0.3234\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 61s 759ms/step - loss: 0.0250 - acc: 0.9941 - val_loss: 6.8456 - val_acc: 0.3239\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 61s 763ms/step - loss: 0.0245 - acc: 0.9942 - val_loss: 6.8055 - val_acc: 0.3301\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 61s 762ms/step - loss: 0.0249 - acc: 0.9936 - val_loss: 6.8395 - val_acc: 0.3275\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 61s 767ms/step - loss: 0.0253 - acc: 0.9938 - val_loss: 6.8533 - val_acc: 0.3288\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 61s 757ms/step - loss: 0.0243 - acc: 0.9937 - val_loss: 6.8432 - val_acc: 0.3329\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 61s 760ms/step - loss: 0.0249 - acc: 0.9938 - val_loss: 6.8667 - val_acc: 0.3319\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 60s 755ms/step - loss: 0.0244 - acc: 0.9941 - val_loss: 6.8624 - val_acc: 0.3358\n"
     ]
    }
   ],
   "source": [
    "with active_session():\n",
    "    # do long-running work here\n",
    "    model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                        steps_per_epoch = train_samples//batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                        validation_steps = test_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model.save_weights('nmt_weights_100epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model info\n",
    "model.load_weights('nmt_weights_100epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"Context vectors\"\n",
    "\n",
    "#CREATE THE ENCODER\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_state_input)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_state_input,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first character of \n",
    "    #target sequence with the start character.\n",
    "    target_seq[0, 0] = target_word2idx['<sos>']\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word =target_idx2word[sampled_token_index]\n",
    "        decoded_sentence += ' '+ sampled_word\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '<eos>' or len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Source sentence: <sos> they hated tom <eos>\n",
      "Actual Target Translation: كانوا يكرهون توم <\n",
      "Predicted Target Translation:  كانوا يكرهون توم <\n",
      "\n",
      "\n",
      "\n",
      "Input Source sentence: <sos> you never listen i might as well talk to the wall <eos>\n",
      "Actual Target Translation: لا تُنْصِتُ أبدًا ربما يَحْسُنُ بي التحدث إلى الجدار <\n",
      "Predicted Target Translation:  هذا تخصصه <\n"
     ]
    }
   ],
   "source": [
    "#predicting from training set and test set\n",
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1\n",
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input Source sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Target Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Target Translation:', decoded_sentence[:-4])\n",
    "print ('\\n\\n')\n",
    "test_gen = generate_batch(X_test, y_test, batch_size = 1)\n",
    "k=100\n",
    "k+=1\n",
    "(input_seq, actual_output), _ = next(test_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input Source sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Target Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Target Translation:', decoded_sentence[:-4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
